
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Deepak Pathak, Jon Barron and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 19px;
    font-weight: 1000
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 800
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  </style>
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Cliff Ahn</title>
  <meta name="Cliff Ahn's Homepage" http-equiv="Content-Type" content="Cliff Ahn's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-99756592-1', 'auto');
    ga('send', 'pageview');
  </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
</head>

<body>
<table width="840" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center"><font size="7">Cliff Ahn</font><br>
    <b>Email</b>:
    <font id="email" style="display:inline;">
      <noscript><i>Please enable Javascript to view</i></noscript>
    </font>
    <script>
    emailScramble = new scrambledString(document.getElementById('email'),
        'emailScramble', 'edkceyfufn@bilaeehl.r',
	[13, 19, 12, 0, 10, 16, 3, 20, 4, 7, 8, 9, 2, 1, 5, 18, 15, 6, 14, 17, 11]);
    </script>
  </p>

  <tr>
    <td width="67%" valign="middle" align="justify">
    <p>I'm a business strategist with a passion for turning untapped engagement into tangible results, connecting technology companies to communities they otherwise wouldn't be able to reach. I identify and cultivate relationships, moving people from brand-aware to brand-advocate, while advising company executives on how to best leverage these community assets to drive growth and respond to feedback.</p>
    
    <p>I received a BA in Interdisciplinary Studies at <a target="_blank" href="http://www.berkeley.edu/">UC Berkeley</a>, culminating in a thesis with a focus on Sociology, Psychology, and Media Studies. I was a TA for the <a target="_blank"href="https://blockchain.berkeley.edu/courses/fall-2017-fundamentals-decal/">Blockchain Fundamentals Decal</a>, a leader at <a target="_blank" href="https://blockchain.berkeley.edu/">Blockchain at Berkeley</a>, and have been volunteering as the head of logistics for <a target="_blank" href="https://she256.org/">she256</a> since its inception. I have experience building organizations, events, and teams from the ground up and love working cross-functionally with diverse groups and individuals.</p>
    
    <p>My passions lie in areas of social good, diversity, equity, and inclusion for I believe that it is only through the uplifting of all that we achieve a better society. Innovation is a process, so is the pursuit of justice. Constant proactive improvement is the only path forward.</p>

    <p>American-Born, Korean. Berkeley, CA. Fiat Lux.</p>
    
    <p align=center>
    <a target="_blank" href="cliff_cv.pdf">CV</a> | 
    <a target="_blank"href="https://www.linkedin.com/in/cliffahn/"> LinkedIn </a>|
    <a target="_blank" href="http://www.github.com/raulpuric"> Thesis </a>
    </p>
    </td>

    <td width="33%"><a target="_blank" href="images/raul_puri.jpg"><img src="images/raul_puri.jpg" width="90%"></a></td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td><sectionheading>Selected Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

  <tr>
    <td width="33%" valign="top" align="center">
	    <img src="images/qgen.PNG" style="width: 200px"></img>

    <td width="67%" valign="top">
      <p><a target="_blank" href="https://arxiv.org/abs/2002.09599" id="QGEN">
      <img src="images/new.png" alt="[NEW]" width="6%" style="border-style: none">
      <heading>Training Question Answering Models From Synthetic Data</heading></a><br>
      <strong>Raul Puri</strong>, Ryan Spring, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro<br>
      <em>arXiv</em>, 2020<br>
      <br></p>

      <div class="paper" id="qgen">
        <a target="_blank" href="https://arxiv.org/abs/2002.09599">pdf</a> | 
        <a href="javascript:toggleblock('qgen_abs')">abstract</a>    
        <p align="justify"> <i id="qgen_abs">Question and answer generation is a data augmentation method that aims to improve question answering (QA) models given the limited amount of human labeled data. However, a considerable gap remains between synthetic and human-generated question-answer pairs. This work aims to narrow this gap by taking advantage of large language models and explores several factors such as model size, quality of pretrained models, scale of data synthesized, and algorithmic choices. On the SQUAD1.1 question answering task, we achieve higher accuracy using solely synthetic questions and answers than when using the SQUAD1.1 training set questions alone. Removing access to real Wikipedia data, we synthesize questions and answers from a synthetic corpus generated by an 8.3 billion parameter GPT-2 model. With no access to human supervision and only access to other models, we are able to train state of the art question answering networks on entirely model-generated data that achieve 88.4 Exact Match (EM) and 93.9 F1 score on the SQUAD1.1 dev set. We further apply our methodology to SQUAD2.0 and show a 2.8 absolute gain on EM score compared to prior work using synthetic data.</i></p>
      </div>
    </td>
  </tr>
  <tr>
    <td width="33%" valign="top" align="center">
        <img src="images/zero_text_clf.PNG" style="width: 200px"></img>

    <td width="67%" valign="top">
      <p><a target="_blank" href="https://arxiv.org/abs/1912.10165" id="ZEROTEXTCLF">
      <heading>Zero-shot Text Classification With Generative Language Models</heading></a><br>
      <strong>Raul Puri</strong>, Bryan Catanzaro<br>
      <em>MetaLearn 2019 @ Neurips</em>, 2019<br>
      <br></p>

      <div class="paper" id="zero_text_clf">
        <a target="_blank" href="https://arxiv.org/abs/1912.10165">pdf</a> | 
        <a href="javascript:toggleblock('zero_text_clf_abs')">abstract</a>    
        <p align="justify"> <i id="zero_text_clf_abs"> This work investigates the use of natural language to enable zero-shot model adaptation to new tasks. We use text and metadata from social commenting platforms as a source for a simple pretraining task. We then provide the language model with natural language descriptions of classification tasks as input and train it to generate the correct answer in natural language via a language modeling objective. This allows the model to generalize to new classification tasks without the need for multiple multitask classification heads. We show the zero-shot performance of these generative language models, trained with weak supervision, on six benchmark text classification datasets from the torchtext library. Despite no access to training data, we achieve up to a 45% absolute improvement in classification accuracy over random or majority class baselines. These results show that natural language can serve as simple and powerful descriptors for task adaptation. We believe this points the way to new metalearning strategies for text problems.
        </i></p>
      </div>
    </td>
  </tr>

  <tr>
    <td width="33%" valign="top" align="center"><img src="images/megatron.PNG" style="width: 160px"></img>
    <td width="67%" valign="top">
      <p><a target="_blank" href="images/Megatron.pdf" id="MEGATRON">
      <heading>Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism</heading></a><br>
      Mohammad Shoeybi*, Mostofa Patwary*,<strong>Raul Puri*</strong>, Patrick LeGresley, Jared Casper, Bryan Catanzaro<br>
      <em>arXiv</em>, 2019<br>
      <br></p>

      <div class="paper" id="ipgan">
        <a target="_blank" href="images/Megatron.pdf">pdf</a> | 
        <a href="javascript:toggleblock('megatron_abs')">abstract</a>    
        <p align="justify"> <i id="megatron_abs">Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (88.8% compared to single model SOTA accuracy of 86.5%). We release our code and the weights of our models for reproducibility. 
        </i></p>
      </div>
    </td>
  </tr>
  <tr>
    <td width="33%" valign="top" align="center">
        <img src="images/large_lm_40gb.PNG" style="width: 200px"></img>

    <td width="67%" valign="top">
      <p><a target="_blank" href="https://arxiv.org/pdf/1808.01371.pdf" id="LARGELM">
      <heading>Large Scale Language Modeling: Converging on 40GB of Text in Four Hours</heading></a><br>
      <strong>Raul Puri</strong>, Robert Kirby, Nikolai Yakovenko, Bryan Catanzaro<br>
      <em>HPML</em>, 2018<br>
      <br></p>

      <div class="paper" id="large_lm_40gb">
        <a target="_blank" href="https://arxiv.org/pdf/1808.01371.pdf">pdf</a> | 
        <a href="javascript:toggleblock('large_lm_40gb_abs')">abstract</a>    
        <p align="justify"> <i id="large_lm_40gb_abs"> Recent work has shown how to train Convolutional Neural Networks (CNNs) rapidly on large image datasets [1], then transfer the knowledge gained from these models to a variety of tasks [2]. Following [3], in this work, we demonstrate similar scalability and transfer for Recurrent Neural Networks (RNNs) for Natural Language tasks. By utilizing mixed precision arithmetic and a 32k batch size distributed across 128 NVIDIA Tesla V100 GPUs, we are able to train a character-level 4096-dimension multiplicative LSTM (mLSTM) [4] for unsupervised text reconstruction over 3 epochs of the 40 GB Amazon Reviews dataset [5] in four hours. This runtime compares favorably with previous work taking one month to train the same size and configuration for one epoch over the same dataset [3]. Converging large batch RNN models can be challenging. Recent work has suggested scaling the learning rate as a function of batch size, but we find that simply scaling the learning rate as a function of batch size leads either to significantly worse convergence or immediate divergence for this problem. We provide a learning rate schedule that allows our model to converge with a 32k batch size. Since our model converges over the Amazon Reviews dataset in hours, and our compute requirement of 128 Tesla V100 GPUs, while substantial, is commercially available, this work opens up large scale unsupervised NLP training to most commercial applications and deep learning researchers. A model can be trained over most public or private text datasets overnight.
        </i></p>
      </div>
    </td>
  </tr>

</table>

<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('qgen_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('zero_text_clf_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('megatron_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('large_lm_40gb_abs');
</script>
</body>

</html>
